#!/usr/bin/env python
# vim: set fileencoding=utf-8 sw=4 ts=4 et :

"""
track-large-files

Keeps track of large files.
Maintains a size -> inode lookup table.
"""

import argparse
import collections
import errno
import fcntl
import hashlib
import os
import sqlalchemy
import stat
import sys
import tracking_model
import xdg.BaseDirectory  # pyxdg, apt:python-xdg

from btrfs import lookup_ino_paths, get_fsid, get_root_generation, clone_data
from dedup import ImmutableFDs, cmp_files
from openat import fopenat, fopenat_rw
from sqlalchemy import func, sql
from sqlalchemy.orm import sessionmaker, aliased
from tracking_model import Filesystem, InodeAndSize, MiniHash, get_or_create

BUFSIZE = 8192


def track_updated_files(sess, volume_fd, results_file):
    from btrfs import ffi, u64_max

    fs, fs_created = get_or_create(
        sess, Filesystem,
        uuid=get_fsid(volume_fd).bytes)
    if fs_created:
        fs.last_tracked_generation = 0

    min_generation = fs.last_tracked_generation
    top_generation = get_root_generation(volume_fd)
    results_file.write(
        'generations %d %d\n' % (min_generation, top_generation))

    args = ffi.new('struct btrfs_ioctl_search_args *')
    args_buffer = ffi.buffer(args)
    sk = args.key
    lib = ffi.verifier.load_library()

    # Not a valid objectid that I know.
    # But find-new uses that and it seems to work.
    sk.tree_id = 0

    # Because we don't have min_objectid = max_objectid,
    # a min_type filter would be ineffective.
    # min_ criteria are modified by the kernel during tree traversal;
    # they are used as an iterator on tuple order,
    # not an intersection of min ranges.
    sk.min_transid = min_generation

    sk.max_objectid = u64_max
    sk.max_offset = u64_max
    sk.max_transid = u64_max
    sk.max_type = lib.BTRFS_INODE_ITEM_KEY

    while True:
        sk.nr_items = 4096

        try:
            fcntl.ioctl(
                volume_fd, lib.BTRFS_IOC_TREE_SEARCH, args_buffer)
        except IOError as e:
            raise

        if sk.nr_items == 0:
            break

        offset = 0
        for item_id in xrange(sk.nr_items):
            sh = ffi.cast(
                'struct btrfs_ioctl_search_header *', args.buf + offset)
            offset += ffi.sizeof('struct btrfs_ioctl_search_header') + sh.len

            # We can't prevent the search from grabbing irrelevant types
            if sh.type == lib.BTRFS_INODE_ITEM_KEY:
                item = ffi.cast(
                    'struct btrfs_inode_item *', sh + 1)
                found_gen = lib.btrfs_stack_inode_generation(item)
                size = lib.btrfs_stack_inode_size(item)
                mode = lib.btrfs_stack_inode_mode(item)
                if size < SIZE_CUTOFF:
                    continue
                if found_gen < min_generation:
                    continue
                if not stat.S_ISREG(mode):
                    continue
                inode = sh.objectid
                ias, created = get_or_create(
                    sess,
                    InodeAndSize,
                    fs=fs,
                    inode=inode,
                    size=size)
                names = list(lookup_ino_paths(volume_fd, inode))
                results_file.write(
                    'item type %d inode %d len %d'
                    ' gen0 %d gen1 %d size %d names %r mode %o\n' % (
                        sh.type, inode, sh.len,
                        sh.transid, found_gen, size, names,
                        mode))
        sk.min_objectid = sh.objectid
        sk.min_type = sh.type
        sk.min_offset = sh.offset

        sk.min_offset += 1
    fs.last_tracked_generation = top_generation
    sess.commit()

    # Rename the entity to be able to self-join unambiguously
    IAS2 = aliased(InodeAndSize)

    for ias in sess.query(
        IAS2
    ).select_from(
        sess.query(
            InodeAndSize.size
        ).filter_by(
            fs=fs
        ).group_by(
            InodeAndSize.size
        ).having(
            func.count(InodeAndSize.inode) > 1
        ).subquery()
    ).filter(
        IAS2.size == InodeAndSize.size
    ):
        # XXX Need to cope with deleted inodes.
        # We cannot find them in the search-new pass,
        # not without doing some tracking of directory modifications to poke
        # updated directories to find removed elements.

        # We need to set both fs_id, inode, and ias, despite the redundancy;
        # ias isn't set reasonably in the mh_created case.
        mini_hash, mh_created = get_or_create(
            sess, MiniHash, fs_id=ias.fs_id, inode=ias.inode, ias=ias)

        # rehash everytime for now
        # I don't know enough about how inode transaction numbers
        # are updated (as opposed to extent updates)
        # to be able to actually cache the result
        try:
            paths = list(lookup_ino_paths(volume_fd, ias.inode))
        except IOError, e:
            if e.errno != errno.ENOENT:
                raise
            # We have a stale record for a removed inode
            # XXX If an inode number is reused and the second instance is
            # below the size cutoff, we won't update the InodeAndSize
            # record and we won't get an IOError to notify us either.
            # Inode reuse does happen (with and without inode_cache),
            # so this branch isn't enough to rid us of all stale entries.
            if mh_created:
                sess.expunge(mini_hash)
            else:
                sess.delete(mini_hash)
            sess.delete(ias)
            continue
        rfile = fopenat(volume_fd, paths[0])
        mini_hash.update_from_file(rfile)
    sess.commit()

    for (mini_hash, size, inodes) in sess.query(
        MiniHash.mini_hash,
        InodeAndSize.size,
        func.group_concat(MiniHash.inode)
    ).filter(
        InodeAndSize.inode == MiniHash.inode,
        InodeAndSize.fs_id == MiniHash.fs_id,
        InodeAndSize.fs == fs,
    ).group_by(
        InodeAndSize.size,
        MiniHash.mini_hash
    ).having(
        func.count(InodeAndSize.inode) > 1
    ):
        inodes = map(int, inodes.split(','))
        results_file.write(
            'dupe candidates for size %d and mini_hash %x\n'
            % (size, mini_hash))
        files = []
        fds = []
        fd_names = {}
        by_hash = collections.defaultdict(list)

        for inode in inodes:
            paths = list(lookup_ino_paths(volume_fd, inode))
            results_file.write('inode %d paths %s\n' % (inode, paths))
            # Open everything rw, we don't know which
            # can be a read-only source yet.
            afile = fopenat_rw(volume_fd, paths[0])
            fd_names[afile.fileno()] = paths[0]
            files.append(afile)
            fds.append(afile.fileno())

        with ImmutableFDs(fds) as immutability:
            for afile in files:
                afd = afile.fileno()
                if afd in immutability.fds_in_write_use:
                    aname = fd_names[afd]
                    results_file.write('File %r is in use, skipping' % aname)
                    continue
                hasher = hashlib.sha1()
                for buf in iter(lambda: afile.read(BUFSIZE), ''):
                    hasher.update(buf)
                by_hash[hasher.digest()].append(afile)
            for fileset in by_hash.itervalues():
                if len(fileset) < 2:
                    continue
                sfile = fileset[0]
                dfiles = fileset[1:]
                for dfile in dfiles:
                    sfd = sfile.fileno()
                    dfd = dfile.fileno()
                    sname = fd_names[sfd]
                    dname = fd_names[dfd]
                    if not cmp_files(sfile, dfile):
                        # Probably a bug since we just used a crypto hash
                        results_file.write('Files differ: %r %r\n' % (
                            sname, dname))
                        assert False, (sname, dname)
                        continue
                    results_file.write('Will dedup: %r %r\n' % (
                        sname, dname))
                    clone_data(dest=dfd, src=sfd)

    impact_q1 = sess.query(
        (
            (func.count(InodeAndSize.inode) - 1) * InodeAndSize.size
        ).label('impact')
    ).filter(
        InodeAndSize.inode == MiniHash.inode,
        InodeAndSize.fs_id == MiniHash.fs_id == fs.id,
    ).group_by(
        InodeAndSize.size,
        MiniHash.mini_hash
    ).having(
        # Not strictly necessary, singletons would be *0 above anyway
        func.count(InodeAndSize.inode) > 1
    )
    impact_q2 = sess.query(
        func.sum(sql.literal_column('impact'))
    ).select_from(impact_q1.subquery())
    if False:
        potential_space_gain = sum(i for (i,) in impact_q1)
    else:
        potential_space_gain = impact_q2.scalar()
    results_file.write('Potential space gain: %d\n' % potential_space_gain)


parser = argparse.ArgumentParser(usage=__doc__.strip())
parser.add_argument('volume', help='volume to search')
opts = parser.parse_args()

APP_NAME = 'track-large-files'
DATA_DIR = xdg.BaseDirectory.save_data_path(APP_NAME)

url = sqlalchemy.engine.url.URL(
    'sqlite', database=os.path.join(DATA_DIR, 'db.sqlite'))
engine = sqlalchemy.engine.create_engine(url, echo=False)
Session = sessionmaker(bind=engine)
sess = Session()
tracking_model.META.create_all(engine)

volume_fd = os.open(opts.volume, os.O_DIRECTORY)
# 32MiB, initial scan takes about 12', might gain 15837689948,
# sqlite takes 256k
SIZE_CUTOFF = 32 * 1024 ** 2
# about 12' again, might gain 25807974687
SIZE_CUTOFF = 16 * 1024 ** 2
# 13'40" (36' with a backup job running in parallel), might gain 26929240347,
# sqlite takes 758k
SIZE_CUTOFF = 8 * 1024 ** 2

# May raise IOError, let Python print it
track_updated_files(sess, volume_fd, sys.stdout)

